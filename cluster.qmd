# Working on a `slurm` cluster

In this section we'll look at how to adapt your snakefile to run well across many nodes of a cluster. We'll look at

  1. The `resources`` object
  2. The `params` object
  3. The command-line options for `snakemake` on the cluster
  4. Custom functions for filenames


## How `snakemake` expects to run on a `slurm` cluster

Briefly, `snakemake` expects each job to run individually on different machines on the cluster under the management of one core job that runs for the duration of the pipeline. That means that each job can have its own resources and jobs will dispatch more quickly if they get the correct resources for their needs. We will learn how to set the jobs resources through the `resources` object in the rule.


`params` is a rule attribute, like `input` and `output` that can take parameters to be passed through to the `shell` command run by that rule. It also can be referenced in the command-line invocation of snakemake. This makes it perfect for setting extra job options. Lets look at some examples of use, first just passing an option to a command

## `params`

### Keeping the rule clean 

This is mostly a way to be explicit and make parameters easy to see and rules clean.  Use the new `params` block like the wildcards

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1"),
    fq2=lambda wildcards: my_function(wildcards, "fq2"),
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  params:
    quality=25,
    flags=3
  shell:"""
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | \
samtools view -S -h -b -q {params.quality} -f {params.flags} > {output}
"""
```

### Dynamic parameter setting

We can use `lambda` functions to generate values for parameters if we need to based on the values of wildcards, here we randomly obtain a sub-sampling seed

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1"),
    fq2=lambda wildcards: my_function(wildcards, "fq2"),
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  params:
    seed=lambda: wildcards: guess_parameter(wildcards),
    quality=25,
    flags=3
  shell:"""
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | \
samtools view -S -h -b -q {params.quality} -f {params.flags} --subsample-seed {params.seed} > {output}
"""
```

Note the value at `params` can be additional files, output directories not required for downstream jobs and can be assigned from the `config.yaml`.

### Using `resources` to set `slurm` job options

The newer versions of `snakemake` now utilise "profiles" which allow a user to define a default set of parameters and specify if and what type of cluster is being utilised. Therefore, when working on `slurm` it is required that a "workflow profile" is provided.  

A basic outline of this file is shown below

```
executor: slurm
jobs: 100
use-singularity: true  # change if you want to used sourced tools
printshellcmds: true
keep-going: true
rerun-incomplete: true
latency-wait: 60

# Default resources for all rules
default-resources:
  slurm_partition: "tsl-short"
  mem_mb: 16000
  runtime: 30
  slurm_account: "tsl"  # This must be "tsl" for SLURM

# Default threads if not specified in rule
set-threads:
  __default__: 4  # Most of the rules use 1-4 threads, can set a default

```
Many of the parameters outlined above are self explanatory but note the options to provide default resources and threads.  This can be helpful if a number of your rules use similar settings.

To run `snakemake` with a profile file, see below

```
snakemake --snakefile my.snakefile --configfile config.yaml --workflow-profile workflow_profile.yaml
```

If a user wishes to provide specific `slurm` resources for a certain rule, then this can be provided by using the `resources` attribute.  In practice this means that the job associated with a rule will get its own specific value of `mem`, `partition` and any other `slurm` options, from its `resources` block, allowing the user to specify the value as needed. 

Below is an example of using the `resources` attribute to assign `mem`, `partition` and `wc-key` (the value of which is located in the `config.yaml`).  Note when assigning the number of cores (threads), then the attribute `threads` is used.

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  threads: 8
  resources:
    slurm_extra = f"--wckey={config['wckey']}",
    partition: "tsl-medium",
    mem_mb = 12000
  params:
    seed=lambda: wildcards: guess_parameter(wildcards),
    quality=25,
    flags=3
  shell:"""
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | \
samtools view -S -h -b -q {params.quality} -f {params.flags} --subsample-seed {params.seed} > {output}
"""
```


## `source`

On a cluster like the one TSL uses each job in the snakemake pipeline runs on a machine with its own execution environment, that is its own memory, CPUs and loaded software. This means that things like `source` and `singularity` images have to be loaded in each job and not globally. They wont work if you put them in the command line, instead they have to go in the `shell` block (or script) like this

```{python}
#| eval: false
  shell:"""
source minimap2-2.5
source samtools-1.9
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | \
samtools view -S -h -b -q {params.quality} -f {params.flags} > {output}
"""
```


## Using a file-of-files as a database for mapping wildcards to filesystem paths

Often we will want to use filenames that have no indication of our sample name or other wildcards in them. This might be because they are raw datafiles from sequencing providers and we don't want to change the filenames or copy the large files across the filesystem from a central storage. Because we are able to use `lambda` functions in `snakemake` and any old Python we can make a database of mappings between the wildcard names and other things like filepaths in a csv file, and read it in for use at any time we like.

Consider the following sample file (name `sample_info.csv`)

```
sample, fq1_path, fq2_path, treatment, time
pputida, /my/seq/db/pputida_1.fq, /my/seq/db/pputida_2.fq, test, 0h
ecoli, /my/seq/db/ecoli_mega_R1.fastq.gz, /my/seq/db/ecol_mega_R2_fastq.gz, control, 6h
...
```

Note that the file names don't have a common pattern, so won't easily be resolved by `snakemake` wildcards. Instead we can build lists of the columns by parsing the file in a usual Python way at the top of the `snakemake` file


```{python}
#| eval: false
samples = []
fq1 = []
fq2 = []
times = []
treatments = []
with open("sample_info.csv") as csv:
    for l in csv:
        l = l.rstrip("\n")
        if not l.startswith("sample"):
            els = l.split(",")
            samples.append( els[0] )
            fq1.append( els[1] )
            fq2.append( els[2] )
            times.append( els[3] )
            treatments.append( els[4])
```

We can generate functions that given a `sample` will return the other items e.g `fq`

```{python}
#| eval: false
def sample_to_read(sample, samples, fq):
'''given a sample and list of samples and a list of fqs returns the fq with the same index
as the sample'''
    return fq[samples.index(sample)]
```

So now we can use the wildcard to get back the fq file in the `lambda` function in the rule like this
```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: sample_to_read(wildcards.sample, samples, fq1)
    fq2=lambda wildcards: saample_to_read(wildcards.sample, samples, fq2)

```

Which returns the full filesytem path for each fq based on the `sample` wildcard.

This is a really useful feature, but it can be tempting to think of it as a solution to everything. Try to use it only for files that come _into_ the `snakemake` pipeline at the beginning and not for things that are generated internally or for final outputs.

## Setting the log file and naming the job

It is a good idea to explicitly set the log filename. Otherwise the run log info will go to a generically named slurm output file. This is a problem because every job run on the HPC under `snakemake` generates a slurm output file which is generically named and the main one can get lost. Similarly, the output from the slurm `squeue` command can get busy with many jobs, so it can also be a good idea to set the main job's name. Logfile can be set with the `sbatch` option `-o` and name with `-J` e.g

```
sbatch -J my_jobs -o my_jobs.log ...
```

## Assigning the maximum number of parallel jobs

You can limit the number of jobs that will run concurrently with `-j`. `snakemake` will not allow more than the specified number of jobs into the queue at any one time. It will manage the submission of jobs right until the completion of the pipeline whatever value you choose. It doesn't create any extra work for you, just throttles `snakemake` should you require it. EG

```
snakemake --snakefile my_pipeline.snakefile --j 20
```

## Waiting for the filesystem to catch up

In a HPC environment we sometimes have to wait for processes to finish writing to disk. These operation can be considered complete by the operating system but still need writing or the filesystem fully updated. So if a new process in a pipeline can't find the output its expecting from a finished process becauce the filesystem is behind, the whole thing could fall over. To avoid this we can set a latency time in which the `snakemake` process will wait and keep checking for the file to arrive before crashing out. Ususally 60 seconds is fine. Set it as follows

```
snakemake --snakefile my_pipeline.snakefile --latency-wait 60
```

## Unlocking a crashed process

Occasionally the `snakemake` pipeline will crash, often because one of its dependent jobs has failed to complete properly (perhaps it ran out of memory). In this state `snakemake` will become locked, to prevent further corruption of the pipeline. The next step is for you to check the logs to see what went wrong and manually resolve it. Then (and only then) can you unlock the `snakemake` pipeline and restart it. Thankfully, `snakemake` will pick up from where it left off, so no earlier progress will be lost.

You can unlock with the `snakemake` option `--unlock`, e.g

```
snakemake --snakefile my_pipeline.snakefile --unlock
```

## Creating a dispatch script

With all these things to remember for the cluster it can be useful to write a master dispatch script to hold the `snakemake` and cluster options. Here's an example one that allows to call it in one of the following three ways

  1. `bash do_snake.sh`
  2. `bash do_snake.sh dryrun`
  3. `bash do_snake.sh unlock`
  
`1.` Let's you run the pipeline proper, `2.` does the dryrun, `3` will unlock a crashed process.

Here's what that example script looks like

```
if [ -z "$1" ]
then
    sbatch -J my_job \
    -o my_job.log \
    --wrap="source snakemake_x.x.x; snakemake --snakefile my_pipeline.snakefile my_main_rule --cluster 'sbatch --partition={params.queue} -c {threads} --mem={params.mem}' \ 
    -j 20 \
    --latency-wait 60" 
elif [ $1 = 'unlock' ]
then
    sbatch -J unlock \
        -o my_job.log \
        --wrap="snakemake_x.x.x; snakemake --snakefile my_pipeline.snakefile --unlock" \
        --partition="tsl-short" \
        --mem="16G"
elif [ $1 = "dryrun" ]
then
    sbatch -J dryrun \
    -o my_job.log \
    --wrap="source snakemake_x.x.x; snakemake --snakefile my_pipeline.snakefile -n" \
    --partition="tsl-short" \
    --mem="16G"
fi
```

Note that `snakemake` will need loading with `source`.


## Organising the `snakemake` bits and pieces

If you are going to end up with a pipeline with lots of steps and subscripts and a dispatch script, it can be a good idea to organise into a project structure. Consider putting the scripts and results in separate directories and temp files into scratch as discussed in the `config file` section. Then consider the top level directory as the base for executing everything. Something like this would be good

```
$ pwd 
my_pipeline 
$ tree
.
├── README.txt
├── config.yaml
├── results
└── scripts
    ├── do_pipeline.sh
    └── my_pipeline.snakemake
```

so that when you're in the `my_pipeline` directory everything can be run as e.g `bash scripts/do_pipeline.sh dryrun`

