# Useful `snakemake` features

## The `expand()` function

`snakemake` requires a list of files in it's rule inputs. These are just standard Python lists and can be made using functions. A helper function called `expand()` does some wildcard expansion of its own. You can see its use in our `final_alignments` rule here.

```{python}
#| eval: false
samples = ['ecoli', 'pputida']

rule final_alignments:
  input: expand( "{sample}.sorted.bam", sample=samples)
```

We can create all the input files programatically using a list of names `samples` and using the `expand()` function which just slots each of the values into its proper place to create a list, saving us a lot of definitions on large sample sets. This will work the same if we give it more than one list and wildcard to expand, like this



```{python}
#| eval: false
samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

rule final_alignments:
  input: expand( "{sample}_{time}_{treatment}.sorted.bam", sample=samples, time=timepoints, treatment=treatments)
```

which will create all the combinations of those lists.

## The `config.yml` file

We won't often have all our files in the current directory, nor want our results and intermediate files to go there, they'll usually be spread about the filesystem. Which means we will have to start dealing with varied paths. Recall that `snakemake` _is_ Python. This means that we can create paths using standard Python functions, like `os.path.join()`. This is most useful when combined with a `config.yaml` file which looks something like this

```
scratch: "/path/to/a/scratch/folder/"
databases: "/path/to/a/database/folder/"
results: "/path/to/a/results/folder"
```

These paths make up a base set of paths that we may want to write or read from in our rules. When loaded into the snakefile a Python `dict` object called `config` is created that we can access using the keys named in `config.yaml`. 

Here's an example:

```{python}
#| eval: false

samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

configfile: "config.yml"

RESULTS_DIR = config["results"]
SCRATCH_DIR = config["scratch"]
DATABASE_DIR = config["databases"]

rule final_alignments:
  input: expand(os.path.join(RESULTS_DIR,  "{sample}_{time}_{treatment}.sorted.bam", sample=samples, time=timepoints, treatment=treatments))
    
rule sort:
  input: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  output: os.path.join(RESULTS_DIR, "{sample}_{time}_{treatment}.sorted.bam")
  shell: "samtools sort {input} -o {output}"

rule align_and_bam:
  input:
    fq1="{sample}_{time}_{treatment}_left_R1.fq",
    fq2="{sample}_{time}_{treatment}_right_R2.fq",
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | samtools view -S -h -b -q 25 -f 3 > {output}"

```

It should be easy to see how to load the config file and inject the values into our paths nicely. 


## `lambda` functions

In the `config` example above it may have been conspicuous that the fastq files were not graced with the information from the config file. This gives us opportunity to explore how to use the wildcard information to get a path using custom functions. For input files, `snakemake` allows us to use a Python `lambda` function. These are one line functions that don't get a name. You can pass them the `wildcards` object and get them to call a second function that uses that information to generate the pathname for the file. Have a look at this snippet

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")

```

The function `my_function()` _must_ return a single pathname as a string, as it is _just_ Python the function can be defined in the top of the `snakemake` file or imported.  We'll look at these in more depth later. 

## Logging specific steps

It is important to generate logs for our snakemake rules and steps within, as this will make troubleshooting easier.  To do this you can add the `log` attribute to each rule.  Note it is also required that you add to the shell line after your job commands the following: `2> {log}` 

It is fine to include this even if your output uses `>` to redirect stdout, as `2>` redirects stderror.


```{python}
#| eval: false

samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

configfile: "config.yml"

RESULTS_DIR = config["results"]
SCRATCH_DIR = config["scratch"]
DATABASE_DIR = config["databases"]

# We add a location to store our logs
LOG_DIR = os.path.join(RESULTS_DIR, "logs")

rule final_alignments:
  input: expand(os.path.join(RESULTS_DIR,  "{sample}_{time}_{treatment}.sorted.bam", sample=samples, time=timepoints, treatment=treatments))
    
rule sort:
  input: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  output: os.path.join(RESULTS_DIR, "{sample}_{time}_{treatment}.sorted.bam")
  log: os.path.join(LOG_DIR, "sort_{sample}_{time}_{treatment}.log")
  shell: "samtools sort {input} -o {output} 2> {log}"

rule align_and_bam:
  input:
    fq1="{sample}_{time}_{treatment}_left_R1.fq",
    fq2="{sample}_{time}_{treatment}_right_R2.fq",
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  log: 
    minimap=os.path.join(LOG_DIR, "minimap_{sample}_{time}_{treatment}.log"),
    samtools=os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}"

```


## Rerunning a specific step

If we really want to micro-manage our pipeline we can run individual steps at will. We have up to now been running the whole thing from the final rule. But any rule can be taken as the end point. Just use its name in the invocation,

```
./src/run_workflow.py --rule <any rule>
```

## Deleting intermediate files

Quite often there's no need to keep anything but the final result file(s). Since we can regenerate intermediate files easily using its rule in the   `snakefile` we can usually just tell `snakemake` to remove output files when they're no longer needed by wrapping the path in the `temp()` function, like this

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}"
```

This saves as lot of space during runtime for big pipelines _and_ saves a lot of clean up. 

## More `shell`

In all our examples we've used a `shell` line to hold the command. We can make the `shell` command multi-line by wrapping it in Python triple quotes, enabling us to have longer commands/chains in the snakefile

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell:"""
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | \ samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}
"""
```

A common alternative that prevents the snakefile from getting gummed up with job specifics is just to put the commands in a bash script and call that. Any script that can be run on the command line can be run this way, including Python, R etc

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASES_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell:"bash scripts/do_alignments.sh {input.ref} {input.fq1} {input.fq2}"
```

The `shell:` can also be replaced with `run:` which allows you to use Python directly in the snakefile.

## Increasing reproducibility and portability

Although you can run scripts to perform the commands, it is much better and more practical to run the tools directly.  It allows users to examine the workflow and immediately identify what commands are being carried out.  In addition, with new best practices, `snakemake` workflows will be built to include `singularity` containers for associated tools.  Thereby removing any need to source software and allow the workflow to be run on any computer that has `singularity` installed.  Most importantly though, if the `snakemake` workflow is re-run it will be using the same software version as used in the original run.


## Drawing the pipeline

It is possible to get `snakemake` to generate a picture of your pipeline, which is great for understanding when things get complicated or showing your boss how involved these things are. We use the `--dag` option in conjunction with `graphviz` (which is installed with the `snakemake` environment). Our handy running script handles this and outputs the generated picture to your specified {RESULTS_DIR}

```
./src/run_workflow.py --dag

```

## Summary

These are all helpful `snakemake` features that will help your snakefile work more easily in a real setting. Most pipelines you develop will use most of these features. 

