# Useful `snakemake` features

## The `expand()` function

`snakemake` requires a list of files in it's rule inputs. These are just standard Python lists and can be made using functions. A helper function called `expand()` does some wildcard expansion of its own. You can see its use in our `final_alignments` rule here.

```{python}
#| eval: false
samples = ['ecoli', 'pputida']

rule final_alignments:
  input: expand( "{sample}.sorted.bam", sample=samples)
```

We can create all the input files programatically using a list of names `samples` and using the `expand()` function which just slots each of the values into its proper place to create a list, saving us a lot of definitions on large sample sets. This will work the same if we give it more than one list and wildcard to expand, like this



```{python}
#| eval: false
samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

rule final_alignments:
  input: expand( "{sample}_{time}_{treatment}.sorted.bam", sample=samples, time=timepoints, treatment=treatments)
```

which will create all the combinations of those lists.

## The `config.yaml` file

We won't often have all our files in the current directory, nor want our results and intermediate files to go there, they'll usually be spread about the filesystem. Which means we will have to start dealing with varied paths. Recall that `snakemake` _is_ Python. This means that we can create paths using standard Python functions, like `os.path.join()`. This is most useful when combined with a `config.yaml` file which looks something like this

```
scratch: "/path/to/a/scratch/folder/"
databases: "/path/to/a/database/folder/"
results: "/path/to/a/results/folder"
```

These paths make up a base set of paths that we may want to write or read from in our rules. When loaded into the snakefile a Python `dict` object called `config` is created that we can access using the keys named in `config.yaml`.  To point `snakemake` to the `config.yaml` we need to have the following within our snakefile:

```
configfile: "path/to/config.yaml"
```
As we are using a template `config.yaml` this has already been defined with the `config.yaml` located within `lib/config.yaml`.

Remember from our `config.yaml` we have the following:

```
scratch: "/tsl/scratch/username"
workdir: "directory_in_scratch"
singularity_image: "tools.sif"
profile_dir: "./profiles"
wckey: "your_wckey"
some_config_parameter: "your_value"
```
As we move through the tutorial we will provide examples of how they apply to our snakemake workflow.

Here's an example using our `scratch`, `workdir` and `some_config_parameter` which has been defined here as `database`:

```{python}
#| eval: false

samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

# Note OUTPUT_DIR is already defined in the snakefile provided
SCRATCH_DIR = config["scratch"]
OUTPUT_DIR = os.path.join(SCRATCH_DIR, config["workdir"])
DATABASE_DIR = os.path.join(SCRATCH_DIR, config["database"])

rule final_alignments:
  input: expand(os.path.join(OUTPUT_DIR,  "{sample}_{time}_{treatment}.sorted.bam"), sample=samples, time=timepoints, treatment=treatments)
    
rule sort:
  input: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  output: os.path.join(OUTPUT_DIR, "{sample}_{time}_{treatment}.sorted.bam")
  shell: "samtools sort {input} -o {output}"

rule align_and_bam:
  input:
    fq1=os.path.join(SCRATCH_DIR,"{sample}_{time}_{treatment}_left_R1.fq"),
    fq2=os.path.join(SCRATCH_DIR,"{sample}_{time}_{treatment}_right_R2.fq"),
    ref=os.path.join(DATABASE_DIR, "{sample}_genome.fa")
  output: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | samtools view -S -h -b -q 25 -f 3 > {output}"

```

It should be easy to see how to load the config file and inject the values into our paths nicely.

One thing to note, by doing this we need to move our sample data.  Up until now this data has been located in our project directory, which isn't good practice.  Examine the snakefile above and determine where you need to move your files to, in order to get the workflow to run as intended.

Another thing you probably noticed was the use of the parameter `database` in our snakefile which wasn't listed in our original `config.yaml`. To define our own parameters in the `config.yaml` we can alter the line that reads "some_config_parameter" to be our required parameter.  In the example above it is the name of the directory storing our genome .fa files within our scratch.  To use the `database` parameter we want to update our `config.yaml` to reflect the new key name:

```
scratch: "/tsl/scratch/username"
workdir: "directory_in_scratch"
singularity_image: "tools.sif"
profile_dir: "./profiles"
wckey: "your_wckey"
database: "database"
```

After updating `config.yaml` we can perform a dry-run and check that all our files are being identified as expected.

## `lambda` functions

In the `config` example above it may have been conspicuous that the fastq files were not graced with the information from the config file. This gives us opportunity to explore how to use the wildcard information to get a path using custom functions. For input files, `snakemake` allows us to use a Python `lambda` function. These are one line functions that don't get a name. You can pass them the `wildcards` object and get them to call a second function that uses that information to generate the pathname for the file. Have a look at this snippet

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")

```

The function `my_function()` _must_ return a single pathname as a string, as it is _just_ Python the function can be defined in the top of the `snakemake` file or imported.  We'll look at these in more depth later. 

## Logging specific steps

It is important to generate logs for our snakemake rules and steps within, as this will make troubleshooting easier.  To do this you can add the `log` attribute to each rule.  Note it is also required that you add to the shell line after your job commands the following: `2> {log}` 

It is fine to include this even if your output uses `>` to redirect stdout, as `2>` redirects stderror.


```{python}
#| eval: false

samples = ['ecoli', 'pputida']
timepoints = ['0h', '2h']
treatments = ['test', 'control']

OUTPUT_DIR = config["workdir"]
SCRATCH_DIR = config["scratch"]
DATABASE_DIR = config["database"]

# We add a location to store our logs
LOG_DIR = os.path.join(OUTPUT_DIR, "logs")

rule final_alignments:
  input: expand(os.path.join(OUTPUT_DIR,  "{sample}_{time}_{treatment}.sorted.bam"), sample=samples, time=timepoints, treatment=treatments)
    
rule sort:
  input: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  output: os.path.join(OUTPUT_DIR, "{sample}_{time}_{treatment}.sorted.bam")
  log: os.path.join(LOG_DIR, "sort_{sample}_{time}_{treatment}.log")
  shell: "samtools sort {input} -o {output} 2> {log}"

rule align_and_bam:
  input:
    fq1=os.path.join(SCRATCH_DIR,"{sample}_{time}_{treatment}_left_R1.fq"),
    fq2=os.path.join(SCRATCH_DIR,"{sample}_{time}_{treatment}_right_R2.fq"),
    ref=os.path.join(DATABASE_DIR, "{sample}_genome.fa")
  output: os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam")
  log: 
    minimap=os.path.join(LOG_DIR, "minimap_{sample}_{time}_{treatment}.log"),
    samtools=os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}"

```

For snakemake specific logs, you will find a hidden directory with the associated log files within your project directory.  These logs can be very helpful for troubleshooting specific rules and associated samples.

```
ls -lah .snakemake/slurm_logs/
total 280K
drwxrwx---  7 taz23vul TSL_20 187 Jul  1 12:36 .
drwxrwx--- 18 taz23vul TSL_20 435 Jul  1 12:36 ..
drwxrwx---  8 taz23vul TSL_20 193 Jun 25 11:52 rule_align_and_bam
drwxrwx---  2 taz23vul TSL_20   0 Jul  1 12:37 rule_sort
```

## Rerunning a specific step

If we really want to micro-manage our pipeline we can run individual steps at will. We have up to now been running the whole thing from the final rule. But any rule can be taken as the end point. Just use its name in the invocation,

```
./src/run_workflow.py --rule <any rule>
```

## Deleting intermediate files

Quite often there's no need to keep anything but the final result file(s). Since we can regenerate intermediate files easily using its rule in the   `snakefile` we can usually just tell `snakemake` to remove output files when they're no longer needed by wrapping the path in the `temp()` function, like this

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASE_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: 
    minimap=os.path.join(LOG_DIR, "minimap_{sample}_{time}_{treatment}.log"),
    samtools=os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell: "minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}"
```

This saves as lot of space during runtime for big pipelines _and_ saves a lot of clean up. 

## More `shell`

In all our examples we've used a `shell` line to hold the command. We can make the `shell` command multi-line by wrapping it in Python triple quotes, enabling us to have longer commands/chains in the snakefile

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASE_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: 
    minimap=os.path.join(LOG_DIR, "minimap_{sample}_{time}_{treatment}.log"),
    samtools=os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell:"""
minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2> {log.minimap} | \
samtools view -S -h -b -q 25 -f 3 > {output} 2> {log.samtools}
"""
```

A common alternative that prevents the snakefile from getting gummed up with job specifics is just to put the commands in a bash script and call that. Any script that can be run on the command line can be run this way, including Python, R etc

```{python}
#| eval: false
rule align_and_bam:
  input:
    fq1=lambda wildcards: my_function(wildcards, "fq1")
    fq2=lambda wildcards: my_function(wildcards, "fq2")
    ref=os.path.join(DATABASE_DIR, "{sample}_genome.fa")
  output: temp(os.path.join(SCRATCH_DIR, "{sample}_{time}_{treatment}_aln.bam"))
  log: 
    minimap=os.path.join(LOG_DIR, "minimap_{sample}_{time}_{treatment}.log"),
    samtools=os.path.join(LOG_DIR, "align_bam_{sample}_{time}_{treatment}.log")
  shell:"bash scripts/do_alignments.sh {input.ref} {input.fq1} {input.fq2}"
```

The `shell:` can also be replaced with `run:` which allows you to use Python directly in the snakefile.

## Increasing reproducibility and portability

Although you can put the shell/run commands in scripts, it is often much more practical to put that bit of code where you can see it in the snakefile.   It allows users to examine the workflow and immediately identify what commands are being carried out.  In addition, with new best practices, `snakemake` workflows will use `singularity` containers or mamba envs as their tool source.  Thereby removing any need to source software and allow the workflow to be run on any computer that has `singularity` installed.  Most importantly though, if the `snakemake` workflow is re-run it will be using the same software version as used in the original run.


## Drawing the pipeline

It is possible to get `snakemake` to generate a picture of your pipeline, which is great for understanding when things get complicated or showing your boss how involved these things are. We use the `--dag` option in conjunction with `graphviz` (which is installed with the `snakemake` environment). Our handy running script handles this and outputs the generated picture to your specified {OUTPUT_DIR}

```
./src/run_workflow.py --dag

```

## Summary

These are all helpful `snakemake` features that will help your snakefile work more easily in a real setting. Most pipelines you develop will use most of these features. 

