[
  {
    "objectID": "checkout.html",
    "href": "checkout.html",
    "title": "5  A Checkout Challenge",
    "section": "",
    "text": "If you’re looking for a task to do in snakemake why not try this experiment on the TSL cluster.\nOn TSL’s sequence database we have this Arabidopsis project. It contains six samples and two run files per sample. Write a snakemake pipeline to\n\nQC the reads with e.g fastqc\nFind SNPs with e.g vcftools\nEstimate read counts/TPM with e.g kallisto\n\nHappy snakemake-ing."
  },
  {
    "objectID": "intro.html#making-a-snakefile",
    "href": "intro.html#making-a-snakefile",
    "title": "1  First Look",
    "section": "1.1 Making a snakefile",
    "text": "1.1 Making a snakefile\nsnakemake is intended to replace the mess of bash scripts you use to run your workflows. So let’s look at converting a simple bash script to snakemake\nminimap2 -ax sr ecoli_genome.fa ecoli_left_R1.fq ecoli_right_R2.fq | \\\nsamtools view -S -h -b -q 25 -f 3 &gt; aln.bam\nsamtools sort aln.bam -o aln.sorted.bam\nThe two commands convert into the following snakemake rules\n\nrule sort:\n  input: 'aln.bam'\n  output: 'aln.sorted.bam'\n  shell: \"samtools sort aln.bam -o aln.sorted.bam\"\n\nrule align_and_bam:\n  input:\n    fq1=\"ecoli_left_R1.fq\",\n    fq2=\"ecoli_right_R2.fq\",\n    ref=\"ecoli_genome.fa\"\n  output: \"aln.bam\"\n  shell: \"minimap2 -ax sr ecoli_genome.fa ecoli_left_R1.fq ecoli_right_R2.fq | samtools view -S -h -b -q 25 -f 3 &gt; aln.bam\"\n\nrule final_alignments:\n    input: 'aln.sorted.bam'\n\nHere are the key points\n\nA snakemake file is composed of rules\nEach rule has (at least) one input file, (at least) one output file and a command for making the output file from the input\nRules are therefore linked into a chain or tree by the files that go in and come out\nThere is an extra final rule that specifies the final result of the pipeline. This rule has no output or command, only inputs\n\nThat is the basis of it!"
  },
  {
    "objectID": "intro.html#running-the-pipeline",
    "href": "intro.html#running-the-pipeline",
    "title": "1  First Look",
    "section": "1.2 Running the pipeline",
    "text": "1.2 Running the pipeline\nWe run the rules by putting them in a file. Usually this is suffixed with .snakefile or .smk to give something like my_pipeline.snakefile or my_pipeline.smk. When we come to run the pipeline snakemake needs us to tell it the name of the rule to run (recall our base rule is final_alignments ), and the snakefile and the cores the pipeline is allowed to use with the -c flag.\n\n1.2.1 The dry-run\nUsually we don’t want to run the pipeline without doing some sort of checking first. This is the purpose of the dry run feature. This allows us to see the jobs that the file specifies, without actually doing them, the snakemake -n flag creates a dry-run. Put together that makes something like\nRemember to use snakemake you need to activate the Mamba environment (which you created during the setup described on the previous page). To do so type:\nsource activate /path/to/snakemake_environment\n\nHowever, as we are using run_workflow.py this step is unnecessary as it will source the environment for you. Therefore, you can simply run the following to produce a dryrun\n./src/run_workflow.py --dry-run\n\nThis will produce the following output:\nRunning command: sbatch --partition tsl-long --wckey wckey -J snakemake_9.3 --wrap=\" source activate ./snakemake_env ; snakemake -s src/workflow.smk --configfile lib/config.yaml --workflow-profile profiles --executor slurm -n \"\nSubmitted batch job 13329947\n\nWe get a lot of output, which can be found within the slurm-XXXXXX.out file, which can be viewed using your screen-based text editor of choice e.g. nano, vim etc.\nnano slurm-13329947.out\nIt can be broken down into a few bits,\n\nThe Summary\nThe Jobs\nThe to-be-done list\n\n\n1.2.1.1 The Summary\nAt the top of the file we are given a summary of the number of times each rule will be run and the resources specified (here just the defaults)\nBuilding DAG of jobs...\nJob stats:\njob                 count    min threads    max threads\n----------------  -------  -------------  -------------\nalign_and_bam           1              1              1\nfinal_alignments        1              1              1\nsort                    1              1              1\ntotal                   3              1              1\n\n\n1.2.1.2 The Jobs\nNext we get a much more granular view, each job is presented with the expected input and output and a reason why it needs running. Usually this will be either missing output files i.e. the output hasn’t been created so the job still needs to run or Input files updated by another job meaning that an input file is newer than an output file (or when it is created it will be) so this file needs updating.\n[Wed Oct 26 16:51:18 2022]\nrule align_and_bam:\n    input: ecoli_left_R1.fq, ecoli_right_R2.fq, ecoli_genome.fa\n    output: aln.bam\n    jobid: 2\n    reason: Missing output files: aln.bam\n    resources: tmpdir=/var/folders/22/kjdvv_k14cj1m6hq5hl527qw0006zc/T\n\n\n[Wed Oct 26 16:51:18 2022]\nrule sort:\n    input: aln.bam\n    output: aln.sorted.bam\n    jobid: 1\n    reason: Missing output files: aln.sorted.bam; Input files updated by another job: aln.bam\n    resources: tmpdir=/var/folders/22/kjdvv_k14cj1m6hq5hl527qw0006zc/T\n\n\n[Wed Oct 26 16:51:18 2022]\nlocalrule final_alignments:\n    input: aln.sorted.bam\n    jobid: 0\n    reason: Input files updated by another job: aln.sorted.bam\n    resources: tmpdir=/var/folders/22/kjdvv_k14cj1m6hq5hl527qw0006zc/T\n\n\n1.2.1.3 The to-be-done list\nThis is a version of the summary outlining the bits of the pipeline that need to complete for everything to be in order.\nJob stats:\njob                 count    min threads    max threads\n----------------  -------  -------------  -------------\nalign_and_bam           1              1              1\nfinal_alignments        1              1              1\nsort                    1              1              1\ntotal                   3              1              1\n\nReasons:\n    (check individual jobs above for details)\n    input files updated by another job:\n        final_alignments, sort\n    missing output files:\n        align_and_bam, sort\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n\n\n\n1.2.2 The Run Proper\nEverything looks good in the dry-run so let’s go ahead and run. Although we haven’t made explicit point of it, this run will happen in the current directory with all files expected to be in and going to the current directory. That looks like this at the moment.\n$ ls -l\ntotal 85136\n-rw-r--r--@ 1 macleand  2006   5205449  2 Jul  2019 ecoli_genome.fa\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_left_R1.fq\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_right_R2.fq\n-rw-r--r--  1 macleand  2006       420 26 Oct 16:50 my_pipeline.snakefile\nRun the pipeline with\n./src/run_workflow.py\nWe get a lot of output to the screen our slurm-XXXXXX.out file. Hopefully at the end we see\nFinished job 0.\n3 of 3 steps (100%) done\nan indication that it has completed everything (if not we’re into some debugging - more on that later). And the working directory looks like this now\n-rw-r--r--  1 macleand  2006   9397482 28 Oct 10:13 aln.bam\n-rw-r--r--  1 macleand  2006   7929255 28 Oct 10:13 aln.sorted.bam\n-rw-r--r--@ 1 macleand  2006   5205449  2 Jul  2019 ecoli_genome.fa\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_left_R1.fq\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_right_R2.fq\n-rw-r--r--  1 macleand  2006       420 26 Oct 16:50 my_pipeline.snakefile\nAll the files we expected to be created have been and are sitting nicely in the directory. Hurray!"
  },
  {
    "objectID": "intro.html#the-first-awesome-thing-about-snakemake",
    "href": "intro.html#the-first-awesome-thing-about-snakemake",
    "title": "1  First Look",
    "section": "1.3 The first awesome thing about snakemake",
    "text": "1.3 The first awesome thing about snakemake\nSo far this has all been very much like a bash script. The snakemake file seems to be just an elaborate reproduction. Now lets have a look at a killer feature that makes snakemake very much more useful than bash scripts - its ability to work out whether all parts of the pipeline are up to date and whether anything needs redoing.\nLet’s look at the dry-run output from the pipeline we just ran.\n$ ./src/run_workflow.py --dry-run\nRunning command: sbatch --partition tsl-long --wckey wckey -J snakemake_9.3 --wrap=\" source activate ./snakemake_env ; snakemake -s src/workflow.smk --configfile lib/config.yaml --workflow-profile profiles --executor slurm -n \"\nSubmitted batch job 13331525\nIf we examine our slurm-XXXXXX.out then we will obtain the following:\nnano slurm-13331525.out\n\nUsing workflow specific profile profiles for setting default command line arguments.\nhost: t384n18\nBuilding DAG of jobs...\nJob stats:\njob  count\n-----  -------\ntotal        0\n\nJob stats:\njob  count\n-----  -------\ntotal        0\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\nWell, that’s reassuring. Nothing need be done. What if a component file changed. Lets look at what happens if an input file is updated. Using touch to update the timestamp on the reference file\n$ sample_data touch ecoli_genome.fa\n$ sample_data ls -l\ntotal 119480\n-rw-r--r--  1 macleand  2006   9397482 28 Oct 10:13 aln.bam\n-rw-r--r--  1 macleand  2006   7929255 28 Oct 10:13 aln.sorted.bam\n-rw-r--r--@ 1 macleand  2006   5205449 28 Oct 10:32 ecoli_genome.fa\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_left_R1.fq\n-rw-r--r--@ 1 macleand  2006  19186649 28 Nov  2019 ecoli_right_R2.fq\n-rw-r--r--  1 macleand  2006       420 26 Oct 16:50 my_pipeline.snakefile\nOne of the source files is now newer than the outputs. What does snakemake now think needs to be done\n$ ./src/run_workflow.py --dry-run\n\nBuilding DAG of jobs...\nJob stats:\njob                 count    min threads    max threads\n----------------  -------  -------------  -------------\nalign_and_bam           1              1              1\nfinal_alignments        1              1              1\nsort                    1              1              1\ntotal                   3              1              1\nit thinks that the whole pipeline needs to be redone! This is the first awesome thing about snakemake - if one of the upstream files is updated (input or output files, it doesn’t matter), the relevant parts of the pipeline will be slated to run again (which in this small pipeline will be everything). snakemake will work out which bits need doing again automatically from the rule descriptions. In large pipelines this is a major time saver and increases reproducibility massively. Bash scripts must be manually managed which leads to more manual errors.\n\n\n\n\n\n\nNote\n\n\n\nI find it really hard to overstate how useful this ability to pick-up-from-where-it-left-off is. It saves an immense amount of checking and redoing and re-issuing of the same commands when something went wrong - especially something catastrophic or hard to detect at the end. Or when you unexpectedly get a new sample and need to add it in, or when your boss wants to change one tiny thing. snakemake insane reproducibility is a huge win for research which is naturally iterative. Its true that there’s a slight learning curve and requires more investment in time at the start of the project, but that is won back in spades later."
  },
  {
    "objectID": "intro.html#the-second-awesome-thing-about-snakemake",
    "href": "intro.html#the-second-awesome-thing-about-snakemake",
    "title": "1  First Look",
    "section": "1.4 The second awesome thing about snakemake",
    "text": "1.4 The second awesome thing about snakemake\nThe next life improving thing about snakemake is how it handles files. Up to now we’ve hardcoded the file names into the rules. That’s not scalable. snakemake provides a clever pattern match facility between the input and output files to match them up between the rules, it cross-references these with actual filenames and fills in the patterns. It provides access to them using special objects called wildcards, input and output. All we need to do is specify the patterns in our rules and the inputs to the master rule (ie final_alignments). In essence, we start by describing the files we want to get out of the pipeline and snakemake works back from there according to our pattern.\nThis is massively easier to see in the rules themselves. Here’s our original set of rules.\n\nrule sort:\n  input: 'aln.bam'\n  output: 'aln.sorted.bam'\n  shell: \"samtools sort aln.bam -o aln.sorted.bam\"\n\nrule align_and_bam:\n  input:\n    fq1=\"ecoli_left_R1.fq\",\n    fq2=\"ecoli_right_R2.fq\",\n    ref=\"ecoli_genome.fa\"\n  output: \"aln.bam\"\n  shell: \"minimap2 -ax sr ecoli_genome.fa ecoli_left_R1.fq ecoli_right_R2.fq | samtools view -S -h -b -q 25 -f 3 &gt; aln.bam\"\n\nrule final_alignments:\n  input: 'aln.sorted.bam'\n\nAnd here’s our new set of rules\n\nrule final_alignments:\n  input: \n    ecoli='ecoli.sorted.bam',\n    pputida='pputida.sorted.bam'\n    \nrule sort:\n  input: \"{sample}_aln.bam\"\n  output: \"{sample}.sorted.bam\"\n  shell: \"samtools sort {input} -o {output}\"\n\nrule align_and_bam:\n  input:\n    fq1=\"{sample}_left_R1.fq\",\n    fq2=\"{sample}_right_R2.fq\",\n    ref=\"{sample}_genome.fa\"\n  output: \"{sample}_aln.bam\"\n  shell: \"minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | samtools view -S -h -b -q 25 -f 3 &gt; {output}\"\n\nThings to note:\n\nThe final rule is at the top - it doesn’t actually matter to snakemake what order the rules are in. In many cases for us it’s easier to have the last rule at the top. Note the common name scheme between the input file names.\nThe {sample} is the wildcard. The {} is a replacement operator, the value of the wildcard will be put in there at runtime.\nThe special {input} and {output} objects can have more than one attribute, so we can have more than one input or output file into a rule.\nWe can thread the replacements into the shell commands to make them generic across samples too.\n\nSo we can hopefully see how the rules link up to each other.\nWhat does the dry-run say in the following folder with multiple samples in there?\n$ ls -l\ntotal 170264\n-rw-r--r--@ 1 macleand  2006   5205449 28 Oct 11:04 ecoli_genome.fa\n-rw-r--r--@ 1 macleand  2006  19186649 28 Oct 11:04 ecoli_left_R1.fq\n-rw-r--r--@ 1 macleand  2006  19186649 28 Oct 11:04 ecoli_right_R2.fq\n-rw-r--r--  1 macleand  2006       479 28 Oct 11:28 multi.snakefile\n-rw-r--r--@ 1 macleand  2006   5205449 28 Oct 11:04 pputida_genome.fa\n-rw-r--r--@ 1 macleand  2006  19186649 28 Oct 11:04 pputida_left_R1.fq\n-rw-r--r--@ 1 macleand  2006  19186649 28 Oct 11:04 pputida_right_R2.fq\n\n\n\n$ ./src/run_workflow.py --dry-run\nJob stats:\njob                 count    min threads    max threads\n----------------  -------  -------------  -------------\nalign_and_bam           2              1              1\nfinal_alignments        1              1              1\nsort                    2              1              1\ntotal                   5              1              1\n\nReasons:\n    (check individual jobs above for details)\n    input files updated by another job:\n        final_alignments, sort\n    missing output files:\n        align_and_bam, sort\nLooks great! snakemake has found all our new samples and increased the number of jobs needed accordingly. The figures below show a graphical version of the pipelines. See how snakemake has made it very easy to jump up in scale."
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  First Look",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nWe’ve seen how the fundamentals of snakemake allow us to build efficient and reproducible pipelines. In the next section we’ll look at features of snakemake that help to power larger and more complex pipelines."
  },
  {
    "objectID": "features.html#the-expand-function",
    "href": "features.html#the-expand-function",
    "title": "2  Useful snakemake features",
    "section": "2.1 The expand() function",
    "text": "2.1 The expand() function\nsnakemake requires a list of files in it’s rule inputs. These are just standard Python lists and can be made using functions. A helper function called expand() does some wildcard expansion of its own. You can see its use in our final_alignments rule here.\n\nsamples = ['ecoli', 'pputida']\n\nrule final_alignments:\n  input: expand( \"{sample}.sorted.bam\", sample=samples)\n\nWe can create all the input files programatically using a list of names samples and using the expand() function which just slots each of the values into its proper place to create a list, saving us a lot of definitions on large sample sets. This will work the same if we give it more than one list and wildcard to expand, like this\n\nsamples = ['ecoli', 'pputida']\ntimepoints = ['0h', '2h']\ntreatments = ['test', 'control']\n\nrule final_alignments:\n  input: expand( \"{sample}_{time}_{treatment}.sorted.bam\", sample=samples, time=timepoints, treatment=treatments)\n\nwhich will create all the combinations of those lists."
  },
  {
    "objectID": "features.html#the-config.yml-file",
    "href": "features.html#the-config.yml-file",
    "title": "2  Useful snakemake features",
    "section": "2.2 The config.yml file",
    "text": "2.2 The config.yml file\nWe won’t often have all our files in the current directory, nor want our results and intermediate files to go there, they’ll usually be spread about the filesystem. Which means we will have to start dealing with varied paths. Recall that snakemake is Python. This means that we can create paths using standard Python functions, like os.path.join(). This is most useful when combined with a config.yaml file which looks something like this\nscratch: \"/path/to/a/scratch/folder/\"\ndatabases: \"/path/to/a/database/folder/\"\nresults: \"/path/to/a/results/folder\"\nThese paths make up a base set of paths that we may want to write or read from in our rules. When loaded into the snakefile a Python dict object called config is created that we can access using the keys named in config.yaml.\nHere’s an example:\n\nsamples = ['ecoli', 'pputida']\ntimepoints = ['0h', '2h']\ntreatments = ['test', 'control']\n\nconfigfile: \"config.yml\"\n\nRESULTS_DIR = config[\"results\"]\nSCRATCH_DIR = config[\"scratch\"]\nDATABASE_DIR = config[\"databases\"]\n\nrule final_alignments:\n  input: expand(os.path.join(RESULTS_DIR,  \"{sample}_{time}_{treatment}.sorted.bam\", sample=samples, time=timepoints, treatment=treatments))\n    \nrule sort:\n  input: os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\")\n  output: os.path.join(RESULTS_DIR, \"{sample}_{time}_{treatment}.sorted.bam\")\n  shell: \"samtools sort {input} -o {output}\"\n\nrule align_and_bam:\n  input:\n    fq1=\"{sample}_{time}_{treatment}_left_R1.fq\",\n    fq2=\"{sample}_{time}_{treatment}_right_R2.fq\",\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\")\n  shell: \"minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | samtools view -S -h -b -q 25 -f 3 &gt; {output}\"\n\nIt should be easy to see how to load the config file and inject the values into our paths nicely."
  },
  {
    "objectID": "features.html#lambda-functions",
    "href": "features.html#lambda-functions",
    "title": "2  Useful snakemake features",
    "section": "2.3 lambda functions",
    "text": "2.3 lambda functions\nIn the config example above it may have been conspicuous that the fastq files were not graced with the information from the config file. This gives us opportunity to explore how to use the wildcard information to get a path using custom functions. For input files, snakemake allows us to use a Python lambda function. These are one line functions that don’t get a name. You can pass them the wildcards object and get them to call a second function that uses that information to generate the pathname for the file. Have a look at this snippet\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\")\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\")\n\nThe function my_function() must return a single pathname as a string, as it is just Python the function can be defined in the top of the snakemake file or imported. We’ll look at these in more depth later."
  },
  {
    "objectID": "features.html#logging-specific-steps",
    "href": "features.html#logging-specific-steps",
    "title": "2  Useful snakemake features",
    "section": "2.4 Logging specific steps",
    "text": "2.4 Logging specific steps\nIt is important to generate logs for our snakemake rules and steps within, as this will make troubleshooting easier. To do this you can add the log attribute to each rule. Note it is also required that you add to the shell line after your job commands the following: 2&gt; {log}\nIt is fine to include this even if your output uses &gt; to redirect stdout, as 2&gt; redirects stderror.\n\nsamples = ['ecoli', 'pputida']\ntimepoints = ['0h', '2h']\ntreatments = ['test', 'control']\n\nconfigfile: \"config.yml\"\n\nRESULTS_DIR = config[\"results\"]\nSCRATCH_DIR = config[\"scratch\"]\nDATABASE_DIR = config[\"databases\"]\n\n# We add a location to store our logs\nLOG_DIR = os.path.join(RESULTS_DIR, \"logs\")\n\nrule final_alignments:\n  input: expand(os.path.join(RESULTS_DIR,  \"{sample}_{time}_{treatment}.sorted.bam\", sample=samples, time=timepoints, treatment=treatments))\n    \nrule sort:\n  input: os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\")\n  output: os.path.join(RESULTS_DIR, \"{sample}_{time}_{treatment}.sorted.bam\")\n  log: os.path.join(LOG_DIR, \"sort_{sample}_{time}_{treatment}.log\")\n  shell: \"samtools sort {input} -o {output} 2&gt; {log}\"\n\nrule align_and_bam:\n  input:\n    fq1=\"{sample}_{time}_{treatment}_left_R1.fq\",\n    fq2=\"{sample}_{time}_{treatment}_right_R2.fq\",\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\")\n  log: \n    minimap=os.path.join(LOG_DIR, \"minimap_{sample}_{time}_{treatment}.log\"),\n    samtools=os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  shell: \"minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2&gt; {log.minimap} | samtools view -S -h -b -q 25 -f 3 &gt; {output} 2&gt; {log.samtools}\""
  },
  {
    "objectID": "features.html#rerunning-a-specific-step",
    "href": "features.html#rerunning-a-specific-step",
    "title": "2  Useful snakemake features",
    "section": "2.5 Rerunning a specific step",
    "text": "2.5 Rerunning a specific step\nIf we really want to micro-manage our pipeline we can run individual steps at will. We have up to now been running the whole thing from the final rule. But any rule can be taken as the end point. Just use its name in the invocation,\n./src/run_workflow.py --rule &lt;any rule&gt;"
  },
  {
    "objectID": "features.html#deleting-intermediate-files",
    "href": "features.html#deleting-intermediate-files",
    "title": "2  Useful snakemake features",
    "section": "2.6 Deleting intermediate files",
    "text": "2.6 Deleting intermediate files\nQuite often there’s no need to keep anything but the final result file(s). Since we can regenerate intermediate files easily using its rule in the snakefile we can usually just tell snakemake to remove output files when they’re no longer needed by wrapping the path in the temp() function, like this\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\")\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\")\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  shell: \"minimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2&gt; {log.minimap} | samtools view -S -h -b -q 25 -f 3 &gt; {output} 2&gt; {log.samtools}\"\n\nThis saves as lot of space during runtime for big pipelines and saves a lot of clean up."
  },
  {
    "objectID": "features.html#more-shell",
    "href": "features.html#more-shell",
    "title": "2  Useful snakemake features",
    "section": "2.7 More shell",
    "text": "2.7 More shell\nIn all our examples we’ve used a shell line to hold the command. We can make the shell command multi-line by wrapping it in Python triple quotes, enabling us to have longer commands/chains in the snakefile\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\")\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\")\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  shell:\"\"\"\nminimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2&gt; {log.minimap} | \\ samtools view -S -h -b -q 25 -f 3 &gt; {output} 2&gt; {log.samtools}\n\"\"\"\n\nA common alternative that prevents the snakefile from getting gummed up with job specifics is just to put the commands in a bash script and call that. Any script that can be run on the command line can be run this way, including Python, R etc\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\")\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\")\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  shell:\"bash scripts/do_alignments.sh {input.ref} {input.fq1} {input.fq2}\"\n\nThe shell: can also be replaced with run: which allows you to use Python directly in the snakefile."
  },
  {
    "objectID": "features.html#increasing-reproducibility-and-portability",
    "href": "features.html#increasing-reproducibility-and-portability",
    "title": "2  Useful snakemake features",
    "section": "2.8 Increasing reproducibility and portability",
    "text": "2.8 Increasing reproducibility and portability\nAlthough you can run scripts to perform the commands, it is much better and more practical to run the tools directly. It allows users to examine the workflow and immediately identify what commands are being carried out. In addition, with new best practices, snakemake workflows will be built to include singularity containers for associated tools. Thereby removing any need to source software and allow the workflow to be run on any computer that has singularity installed. Most importantly though, if the snakemake workflow is re-run it will be using the same software version as used in the original run."
  },
  {
    "objectID": "features.html#drawing-the-pipeline",
    "href": "features.html#drawing-the-pipeline",
    "title": "2  Useful snakemake features",
    "section": "2.9 Drawing the pipeline",
    "text": "2.9 Drawing the pipeline\nIt is possible to get snakemake to generate a picture of your pipeline, which is great for understanding when things get complicated or showing your boss how involved these things are. We use the --dag option in conjunction with graphviz (which is installed with the snakemake environment). Our handy running script handles this and outputs the generated picture to your specified {RESULTS_DIR}\n./src/run_workflow.py --dag"
  },
  {
    "objectID": "features.html#summary",
    "href": "features.html#summary",
    "title": "2  Useful snakemake features",
    "section": "2.10 Summary",
    "text": "2.10 Summary\nThese are all helpful snakemake features that will help your snakefile work more easily in a real setting. Most pipelines you develop will use most of these features."
  },
  {
    "objectID": "cluster.html#how-snakemake-expects-to-run-on-a-slurm-cluster",
    "href": "cluster.html#how-snakemake-expects-to-run-on-a-slurm-cluster",
    "title": "3  Working on a slurm cluster",
    "section": "3.1 How snakemake expects to run on a slurm cluster",
    "text": "3.1 How snakemake expects to run on a slurm cluster\nBriefly, snakemake expects each job to run individually on different machines on the cluster under the management of one core job that runs for the duration of the pipeline. That means that each job can have its own resources and jobs will dispatch more quickly if they get the correct resources for their needs. We will learn how to set the jobs resources through the resources object in the rule.\nparams is a rule attribute, like input and output that can take parameters to be passed through to the shell command run by that rule. It also can be referenced in the command-line invocation of snakemake. This makes it perfect for setting extra job options. Lets look at some examples of use, first just passing an option to a command"
  },
  {
    "objectID": "cluster.html#params",
    "href": "cluster.html#params",
    "title": "3  Working on a slurm cluster",
    "section": "3.2 params",
    "text": "3.2 params\n\n3.2.1 Keeping the rule clean\nThis is mostly a way to be explicit and make parameters easy to see and rules clean. Use the new params block like the wildcards\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\"),\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\"),\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  params:\n    quality=25,\n    flags=3\n  shell:\"\"\"\nminimap2 -ax sr {input.ref} {input.fq1} {input.fq2} | \\\nsamtools view -S -h -b -q {params.quality} -f {params.flags} &gt; {output}\n\"\"\"\n\n\n\n3.2.2 Dynamic parameter setting\nWe can use lambda functions to generate values for parameters if we need to based on the values of wildcards, here we randomly obtain a sub-sampling seed\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\"),\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\"),\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  params:\n    seed=lambda: wildcards: guess_parameter(wildcards),\n    quality=25,\n    flags=3\n  shell:\"\"\"\nminimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2&gt; {log.minimap} | \\\nsamtools view -S -h -b -q {params.quality} -f {params.flags} --subsample-seed {params.seed} &gt; {output} 2&gt; {log.samtools}\n\"\"\"\n\nNote the value at params can be additional files, output directories not required for downstream jobs and can be assigned from the config.yaml.\n\n\n3.2.3 Using resources to set slurm job options\nThe newer versions of snakemake now utilise “profiles” which allow a user to define a default set of parameters and specify if and what type of cluster is being utilised. Therefore, when working on slurm it is required that a “workflow profile” is provided.\nA basic outline of this file is shown below\nexecutor: slurm\njobs: 100\nuse-singularity: true  # change if you want to used sourced tools\nprintshellcmds: true\nkeep-going: true\nrerun-incomplete: true\nlatency-wait: 60\n\n# Default resources for all rules\ndefault-resources:\n  slurm_partition: \"tsl-short\"\n  mem_mb: 16000\n  runtime: 30\n  slurm_account: \"tsl\"  # This must be \"tsl\" for SLURM\n\n# Default threads if not specified in rule\nset-threads:\n  __default__: 4  # Most of the rules use 1-4 threads, can set a default\n\nMany of the parameters outlined above are self explanatory but note the options to provide default resources and threads. This can be helpful if a number of your rules use similar settings.\nTo run snakemake with a profile file, see below\nsnakemake --snakefile my.snakefile --configfile config.yaml --workflow-profile config.v9+.yaml\nNot that this workflow profile needs to be named “config.v9+.yaml” in order for snakemake to recognise it as a profile and utilise the defaults as expected.\nIf a user wishes to provide specific slurm resources for a certain rule, then this can be provided by using the resources attribute. In practice this means that the job associated with a rule will get its own specific value of mem, partition and any other slurm options, from its resources block, allowing the user to specify the value as needed.\nBelow is an example of using the resources attribute to assign mem, partition and wc-key (the value of which is located in the config.yaml). Note when assigning the number of cores (threads), then the attribute threads is used.\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: my_function(wildcards, \"fq1\")\n    fq2=lambda wildcards: my_function(wildcards, \"fq2\")\n    ref=os.path.join(DATABASES_DIR, \"{sample}_genome.fa\")\n  output: temp(os.path.join(SCRATCH_DIR, \"{sample}_{time}_{treatment}_aln.bam\"))\n  log: os.path.join(LOG_DIR, \"align_bam_{sample}_{time}_{treatment}.log\")\n  threads: 8\n  resources:\n    slurm_extra = f\"--wckey={config['wckey']}\",\n    partition: \"tsl-medium\",\n    mem_mb = 12000\n  params:\n    seed=lambda: wildcards: guess_parameter(wildcards),\n    quality=25,\n    flags=3\n  shell:\"\"\"\nminimap2 -ax sr {input.ref} {input.fq1} {input.fq2} 2&gt; {log.minimap} | \\\nsamtools view -S -h -b -q {params.quality} -f {params.flags} --subsample-seed {params.seed} &gt; {output} 2&gt; {log.samtools}\n\"\"\""
  },
  {
    "objectID": "cluster.html#singularity",
    "href": "cluster.html#singularity",
    "title": "3  Working on a slurm cluster",
    "section": "3.3 singularity",
    "text": "3.3 singularity\nOn a cluster like the one TSL uses each job in the snakemake pipeline runs on a machine with its own execution environment, that is its own memory, CPUs and loaded software. This means that things like source and singularity images have to be loaded in each job and not globally.\nsnakemake introduced the ability to run required tools from containers e.g. Docker, Singularity. To enable this function we add the attribute container to the snakefile.\n\ncontainer: config[\"singularity_image\"]\n\nWhere the container location has been defined in the config.yaml.\nFor each snakemake workflow that is built, there should be a specific singularity container for the required software/tools to carry out the analysis the snakefile defines. To build a container you require a defintion file. Often we can use Mamba to install our tools, therefore a template definition file has been written that can be modified for each individuals needs. This can be located on the HPC at:\n/tsl/hpc-scripts/bioinformatics/blank_snake/lib/tools.def\nOnce the defintion file has been modified the container can be built. To build the container the software node must be used, which can be accessed on the HPC by typing:\nsoftware\nOnce within the software node, navigate to the location with your container definition file and type:\nsudo singularity build tools.sif tools.def\nIt is the path to the tools.sif that will be added to the config.yaml to be used by snakemake."
  },
  {
    "objectID": "cluster.html#using-a-file-of-files-as-a-database-for-mapping-wildcards-to-filesystem-paths",
    "href": "cluster.html#using-a-file-of-files-as-a-database-for-mapping-wildcards-to-filesystem-paths",
    "title": "3  Working on a slurm cluster",
    "section": "3.4 Using a file-of-files as a database for mapping wildcards to filesystem paths",
    "text": "3.4 Using a file-of-files as a database for mapping wildcards to filesystem paths\nOften we will want to use filenames that have no indication of our sample name or other wildcards in them. This might be because they are raw datafiles from sequencing providers and we don’t want to change the filenames or copy the large files across the filesystem from a central storage. Because we are able to use lambda functions in snakemake and any old Python we can make a database of mappings between the wildcard names and other things like filepaths in a csv file, and read it in for use at any time we like.\nConsider the following sample file (name sample_info.csv)\nsample, fq1_path, fq2_path, treatment, time\npputida, /my/seq/db/pputida_1.fq, /my/seq/db/pputida_2.fq, test, 0h\necoli, /my/seq/db/ecoli_mega_R1.fastq.gz, /my/seq/db/ecol_mega_R2_fastq.gz, control, 6h\n...\nNote that the file names don’t have a common pattern, so won’t easily be resolved by snakemake wildcards. Instead we can build lists of the columns by parsing the file in a usual Python way at the top of the snakemake file\n\nsamples = []\nfq1 = []\nfq2 = []\ntimes = []\ntreatments = []\nwith open(\"sample_info.csv\") as csv:\n    for l in csv:\n        l = l.rstrip(\"\\n\")\n        if not l.startswith(\"sample\"):\n            els = l.split(\",\")\n            samples.append( els[0] )\n            fq1.append( els[1] )\n            fq2.append( els[2] )\n            times.append( els[3] )\n            treatments.append( els[4])\n\nWe can generate functions that given a sample will return the other items e.g fq\n\ndef sample_to_read(sample, samples, fq):\n'''given a sample and list of samples and a list of fqs returns the fq with the same index\nas the sample'''\n    return fq[samples.index(sample)]\n\nSo now we can use the wildcard to get back the fq file in the lambda function in the rule like this\n\nrule align_and_bam:\n  input:\n    fq1=lambda wildcards: sample_to_read(wildcards.sample, samples, fq1)\n    fq2=lambda wildcards: saample_to_read(wildcards.sample, samples, fq2)\n\nWhich returns the full filesystem path for each fq based on the sample wildcard.\nThis is a really useful feature, but it can be tempting to think of it as a solution to everything. Try to use it only for files that come into the snakemake pipeline at the beginning and not for things that are generated internally or for final outputs."
  },
  {
    "objectID": "cluster.html#assigning-the-maximum-number-of-parallel-jobs",
    "href": "cluster.html#assigning-the-maximum-number-of-parallel-jobs",
    "title": "3  Working on a slurm cluster",
    "section": "3.5 Assigning the maximum number of parallel jobs",
    "text": "3.5 Assigning the maximum number of parallel jobs\nYou can limit the number of jobs that will run concurrently and it is recommended to use the workflow_profile to do this.\njobs: 100\nsnakemake will not allow more than the specified number of jobs into the queue at any one time. It will manage the submission of jobs right until the completion of the pipeline whatever value you choose. It doesn’t create any extra work for you, just throttles snakemake should you require it."
  },
  {
    "objectID": "cluster.html#waiting-for-the-filesystem-to-catch-up",
    "href": "cluster.html#waiting-for-the-filesystem-to-catch-up",
    "title": "3  Working on a slurm cluster",
    "section": "3.6 Waiting for the filesystem to catch up",
    "text": "3.6 Waiting for the filesystem to catch up\nIn a HPC environment we sometimes have to wait for processes to finish writing to disk. These operation can be considered complete by the operating system but still need writing or the filesystem fully updated. So if a new process in a pipeline can’t find the output its expecting from a finished process becauce the filesystem is behind, the whole thing could fall over. To avoid this we can set a latency time in which the snakemake process will wait and keep checking for the file to arrive before crashing out. Ususally 60 seconds is fine. This can be set again using the workflow_profile under:\nlatency-wait: 60\nAs with specifying the number of parallel jobs, it is recommended to set these values using this method."
  },
  {
    "objectID": "cluster.html#unlocking-a-crashed-process",
    "href": "cluster.html#unlocking-a-crashed-process",
    "title": "3  Working on a slurm cluster",
    "section": "3.7 Unlocking a crashed process",
    "text": "3.7 Unlocking a crashed process\nOccasionally the snakemake pipeline will crash, often because one of its dependent jobs has failed to complete properly (perhaps it ran out of memory). In this state snakemake will become locked, to prevent further corruption of the pipeline. The next step is for you to check the logs to see what went wrong and manually resolve it. Then (and only then) can you unlock the snakemake pipeline and restart it. Thankfully, snakemake will pick up from where it left off, so no earlier progress will be lost.\nYou can unlock with the snakemake option --unlock, e.g\nsnakemake --snakefile my_pipeline.snakefile --unlock"
  },
  {
    "objectID": "cluster.html#organising-the-snakemake-bits-and-pieces",
    "href": "cluster.html#organising-the-snakemake-bits-and-pieces",
    "title": "3  Working on a slurm cluster",
    "section": "3.8 Organising the snakemake bits and pieces",
    "text": "3.8 Organising the snakemake bits and pieces\nDuring the setup steps outlined in the Preface there is guidance provided for generating your Project Structure. Following this structure will help organise the required files. Consider putting the scripts and results in separate directories and temp files into scratch as discussed in the config file section. Then consider the top level directory as the base for executing everything. If using the project structure outlined in the Preface then it will look like this\n$ pwd \nmy_pipeline \n$ tree .\n\nmy_pipeline/\n├── lib/\n│   ├── config.yaml        # Configuration file\n│   ├── snakemake_env.yaml # Mamba environment definition\n│   └── tools.def         # Template for Singularity image definition\n├── profiles/\n│   └── config.v9+.yaml   # workflow profile and SLURM-specific settings\n├── src/\n│   ├── run_workflow.py   # Main execution script\n│   └── workflow.smk      # Snakemake workflow rules\n├── VERSION              # Version information\n└── README.md            # This file\nso that when you’re in the my_pipeline directory everything can be run as e.g ./src/run_workflow.py --dry-run\nThe added benefit of using the blank_snake to build your workflow, is that it comes with a handy execution script called run_workflow.py, which, provided the required files are present, will created the sbatch jobs for you.\nIt is recommended you use this script for submitting the snakemake job. With this in mind and remembering we are using our workflow_profile to set defaults e.g. config.v9+.yaml, ensure your default runtime is set for long enough for your whole snakemake run to complete. The master slurm job needs to outlast the duration of all the jobs run within the pipeline.\nThe parameter to look for is under default-resources and “runtime”\n# Default resources for all rules\ndefault-resources:\n  slurm_partition: \"tsl-short\"\n  mem_mb: 16000\n  runtime: 30\n  slurm_account: \"tsl\"  # This must be \"tsl\" for SLURM\n\nThe help options for this script are\n./src/run_workflow.py -h\nusage: run_workflow.py [-h] [--config CONFIG] [--dry-run] [--unlock] [--rule RULE]\n                       [--force] [--version] [--dag]\n\nRun the UNNAMED DRAFT pipeline\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --config CONFIG  Path to config file\n  --dry-run        Perform a dry run\n  --unlock         Unlock a locked directory\n  --rule RULE      Run a specific rule (e.g., generate_report)\n  --force          Force run the rule even if outputs exist\n  --version        Display version information\n  --dag            Generate a PDF of the DAG (workflow_dag.pdf)\n\nExamples:\n  run_workflow --config myproject/config.yaml              # Run pipeline with custom config\n  run_workflow --dry-run                                   # Perform a dry run\n  run_workflow --rule the_second_rule                      # Run until a specific rule\n  run_workflow --unlock                                    # Unlock a snakemake directory\n  run_workflow --version                                   # Display version information\n  run_workflow --dag                                       # Generate DAG visualization\n\nConfiguration:\n  The config file must contain all required parameters defined in this script.\nImportantly we want to ensure we have our config.yaml setup as needed:\nscratch: \"/tsl/scratch/username\" ## Path to your scratch\nworkdir: \"directory_in_scratch\"     ## Directory in scratch to store the results, don't reuse the path to scratch\nsingularity_image: \"tools.sif\"     ## Path to the singularity image to use\nwckey: \"your_wckey\"     ## Your HPC allocation key\nmain_job_partition: \"tsl_short\"     ## The partition to use for the main snakemake job\nsome_config_parameter: \"your_value\"     ## any additional parameters to pass to snakemake"
  },
  {
    "objectID": "summary.html#snakemake-has-lots-of-useful-qualities",
    "href": "summary.html#snakemake-has-lots-of-useful-qualities",
    "title": "4  Summary",
    "section": "4.1 snakemake has lots of useful qualities",
    "text": "4.1 snakemake has lots of useful qualities\nWe’ve seen how to build snakemake pipelines that can handle multiple steps and experimental variables and how to scale it to a cluster or a scattered filesystem. We’ve also seen how snakemake can increase reproducibility and save operator time massively"
  },
  {
    "objectID": "summary.html#but-being-general-over-lots-of-pipelines-isnt-one-of-them",
    "href": "summary.html#but-being-general-over-lots-of-pipelines-isnt-one-of-them",
    "title": "4  Summary",
    "section": "4.2 But being ‘general’ over lots of pipelines isn’t one of them",
    "text": "4.2 But being ‘general’ over lots of pipelines isn’t one of them\nPerhaps at this stage you’d be expecting to see something like ‘how to generalise snakemake’ and re-use it over and over in different projects. That’s not really something snakemake is for. As its really tied to a filesystem then its often a bit fiddly to get to generalise. Instead, take advantage of the fact that snakemake is really good at re-doing stuff. Make as many files as you can temporary with temp() and remove final results. The resulting ‘shell’ of the pipeline could be versioned - perhaps in git - but maybe with something as simple as a datestamp on the project folder. That way you won’t lose that iteration of the pipeline, and it can be re-run exactly should you need to back track."
  },
  {
    "objectID": "summary.html#it-is-worth-the-effort",
    "href": "summary.html#it-is-worth-the-effort",
    "title": "4  Summary",
    "section": "4.3 It is worth the effort",
    "text": "4.3 It is worth the effort\nHopefully this quick intro hasn’t made you think that all this is too much effort. I like to think of tools like snakemake as being things that put you back in charge of the computer. A computer is supposed to be a machine to make your life easier. When we get into a situation where all our work in a task is repetitive then we’ve missed a chance to do that. The learning curve of snakemake is no greater than that you took to learn your first bash script so take the leap and put yourself back in charge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using snakemake to create robust and reproducible bioinformatic pipelines",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Using snakemake to create robust and reproducible bioinformatic pipelines",
    "section": "Motivation",
    "text": "Motivation\nDo you ever feel like the large pipeline or large number of steps in your bioinformatic analysis is a pressure? That somehow it is in charge and you are just there to sit and tell the computer what to do, over and over? Do you fear having missed a step or mis-specified a file and lose sleep from the horror of having to re-do something over again because of a reviewer request? Fear not, these terrors are exactly what snakemake is designed to help slay. snakemake can help you build robust (in the sense that it can be stopped by an unexpected hiccup and can restart from where it left off once that hiccup is cleared) and reproducible pipelines in a quick and easy fashion.\n snakemake is one of a number of tools that allows you to chain together multiple processes into a pipeline. These tools are sometimes called workflow managers and they tie processes together by having some model of the dependency structure between the inputs and outputs of the steps of a pipeline.\nThings like bash scripts can do this job, but they’re bad ways if we want to be reproducible and robust to failure without resorting to heavily engineering scripts to recognise when inputs/outputs change. Dependency based tools like makefiles and their derivatives have been around for decades, doing similar jobs but recently more pipeline specific tools like snakemake, Nextflow Common Workflow Language and even the graphical Galaxy workflows have appeared specifically for scalable reproducible analysis pipelines.\nsnakemake is a good choice as it has a lightweight Python based syntax that will be familiar to many users.\nIn this short tutorial we’ll look at how to create snakemake pipelines for use on a slurm cluster like the one in use at TSL."
  },
  {
    "objectID": "index.html#setup-and-prerequisites",
    "href": "index.html#setup-and-prerequisites",
    "title": "Using snakemake to create robust and reproducible bioinformatic pipelines",
    "section": "Setup and Prerequisites",
    "text": "Setup and Prerequisites\nThis tutorial presumes you are at least a little familiar with bash scripts and Python (but not much) and that you have experience submitting and running jobs on a slurm cluster.\nTo replicate the examples, you’ll need the data here sample_data.zip and to setup your snakemake environment as described here, which will set you up with the latest release. This step is vital as we will be using the script run_workflow.py during this tutorial. Once you have setup your snakemake environment you can use the script install_ok.py within the test directory to confirm everything is setup as required.\nIf you need any help with this please see the bioinformatics team."
  }
]